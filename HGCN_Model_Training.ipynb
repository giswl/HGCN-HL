{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccef8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import *\n",
    "# from src.hgcn_modules import HGCN_Network\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "class HGCN(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        W:Tensor,\n",
    "        bias: bool = True,\n",
    "        W_learn = True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(flow='source_to_target', node_dim=0, **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.W_e = W\n",
    "        if W_learn == True:\n",
    "            self.W_e = Parameter(torch.ones(W.shape[0]),requires_grad=True)\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False,\n",
    "                              weight_initializer='glorot')\n",
    "        self.bias = bias\n",
    "        self.W_learn = W_learn\n",
    "        if self.bias == True:\n",
    "            self.b = Parameter(torch.empty(out_channels))\n",
    "\n",
    "    def forward(self, x: Tensor, hyperedge_index: Tensor):\n",
    "        num_nodes = x.size(0)\n",
    "        num_edges = int(hyperedge_index[1].max()) + 1\n",
    "        hyperedge_weight = x.new_ones(num_edges)\n",
    "        x = self.lin(x)\n",
    "        D_v = scatter(hyperedge_weight[hyperedge_index[1]], hyperedge_index[0],\n",
    "                    dim=0, dim_size=num_nodes, reduce='sum')\n",
    "        D_v = D_v ** -0.5\n",
    "        D_v[D_v == float(\"inf\")] = 0\n",
    "        # print('D_v',D_v)\n",
    "        D_e = scatter(x.new_ones(hyperedge_index.size(1)), hyperedge_index[1],\n",
    "                    dim=0, dim_size=num_edges, reduce='sum')\n",
    "        D_e = 1.0 / D_e\n",
    "        D_e[D_e == float(\"inf\")] = 0\n",
    "\n",
    "        out = self.propagate(hyperedge_index, x=x, norm=D_v, size=(num_nodes, num_edges))\n",
    "        out = D_e.view(-1,1) * out\n",
    "        if self.W_learn == True:\n",
    "            out = torch.mul(self.W_e.view(-1,1),out)\n",
    "        else:\n",
    "            out = self.W_e.mm(out)        \n",
    "        \n",
    "        out = self.propagate(hyperedge_index.flip([0]), x=out,  size=(num_edges, num_nodes))\n",
    "        # print('out1',out)\n",
    "        out = D_v.view(-1,1) * out\n",
    "        # print('out2',out)\n",
    "        if self.bias == True:\n",
    "            out = out + self.b\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, norm_j: Tensor) -> Tensor:\n",
    "        if isinstance(norm_j, Tensor):\n",
    "            out = norm_j.view(-1,1) * x_j\n",
    "        else:\n",
    "            out = x_j\n",
    "        return out\n",
    "\n",
    "class WMF(nn.Module):\n",
    "    \"\"\"\n",
    "    The WMF class implements a simple neural network module that includes batch normalization, a 1x1 convolution, and a LeakyReLU activation function.\n",
    "\n",
    "    Args:\n",
    "        in_dim (int): The number of input channels of the feature map.\n",
    "        out_dim (int): The number of output channels of the feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        # Call the constructor of the parent class nn.Module\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        # Define a 2D batch normalization layer with in_dim input channels\n",
    "        self.BN = nn.BatchNorm2d(in_dim)\n",
    "        # Define a 1x1 2D convolution layer with in_dim input channels and out_dim output channels\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, kernel_size=(1, 1))\n",
    "        # Fix the spelling error and define a LeakyReLU activation function layer\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation method that defines the flow of data through the module.\n",
    "\n",
    "        Args:\n",
    "            X (torch.Tensor): The input feature map.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The processed feature map.\n",
    "        \"\"\"\n",
    "        # Apply batch normalization to the input data\n",
    "        X = self.BN(X)\n",
    "        # Apply a 1x1 convolution operation to the normalized data\n",
    "        X = self.conv(X)\n",
    "        # Apply the LeakyReLU activation function to the convolution result\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "\n",
    "class SSConv(nn.Module):\n",
    "    '''\n",
    "    Spectral-Spatial Convolution\n",
    "    '''\n",
    "    def __init__(self, in_ch, out_ch, kernel_size=3):\n",
    "        super(SSConv, self).__init__()\n",
    "        # Point-wise convolution to transform input channels to output channels\n",
    "        self.point_conv = nn.Conv2d(\n",
    "            in_channels=in_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            groups=1,\n",
    "            bias=False\n",
    "        )\n",
    "        # Initialize point-wise convolution weights\n",
    "        nn.init.kaiming_normal_(self.point_conv.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "\n",
    "        # Depth-wise convolution to extract spatial features\n",
    "        self.depth_conv = nn.Conv2d(\n",
    "            in_channels=out_ch,\n",
    "            out_channels=out_ch,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=kernel_size//2,\n",
    "            groups=out_ch\n",
    "        )\n",
    "        # Initialize depth-wise convolution weights\n",
    "        nn.init.kaiming_normal_(self.depth_conv.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "\n",
    "        # Activation functions for point-wise and depth-wise convolutions\n",
    "        self.pointwise_activation = nn.LeakyReLU()\n",
    "        self.depthwise_activation = nn.LeakyReLU()\n",
    "\n",
    "        # Batch normalization layer for input\n",
    "        self.BN = nn.BatchNorm2d(in_ch)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Apply batch normalization to the input\n",
    "        normalized_input = self.BN(input)\n",
    "        # Apply point-wise convolution\n",
    "        pointwise_output = self.point_conv(normalized_input)\n",
    "        # Apply the first activation function\n",
    "        activated_pointwise = self.pointwise_activation(pointwise_output)\n",
    "        # Apply depth-wise convolution\n",
    "        depthwise_output = self.depth_conv(activated_pointwise)\n",
    "        # Apply the second activation function\n",
    "        final_output = self.depthwise_activation(depthwise_output)\n",
    "        return final_output\n",
    "\n",
    "class HGCN_Network(torch.nn.Module):\n",
    "    def __init__(self, height: int, width: int, in_dim: int,in_dim_Lidar:int, class_num: int,W_e: torch.tensor,lam: float,\n",
    "                 output_dim=128, dropout=0.4):\n",
    "\n",
    "        super(HGCN_Network, self).__init__()\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.in_dim = in_dim\n",
    "        self.in_dim_Lidar = in_dim_Lidar\n",
    "        self.class_num = class_num\n",
    "        self.output_dim = output_dim\n",
    "        self.W_e = W_e\n",
    "        self.dropout = dropout\n",
    "        self.lam = lam\n",
    "\n",
    "        self.WMF_branch = nn.Sequential()\n",
    "        self.WMF_branch.add_module('WMF_branch1',WMF(in_dim, output_dim))\n",
    "        self.WMF_branch.add_module('WMF_branch2',WMF(output_dim, output_dim))\n",
    "\n",
    "        self.HGCN1 = HGCN(in_channels=output_dim, out_channels=output_dim, W=W_e, bias=False)\n",
    "        self.HGCN2 = HGCN(in_channels=output_dim, out_channels=output_dim, W=W_e, bias=False)\n",
    "\n",
    "        self.CNN_Branch = nn.Sequential()\n",
    "        self.CNN_Branch.add_module('CNN_Branch1',SSConv(output_dim, output_dim,kernel_size=5))\n",
    "        self.CNN_Branch.add_module('CNN_Branch2',SSConv(output_dim, output_dim,kernel_size=5))\n",
    "\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.Softmax_linear = nn.Linear(output_dim, class_num)\n",
    "        self.lam = lam\n",
    "        \n",
    "    def forward(self, X,H):\n",
    "\n",
    "        Z = torch.unsqueeze(X.permute([2, 0, 1]), 0)        \n",
    "        Z_WMF = self.WMF_branch(Z)\n",
    "        Z_C = self.CNN_Branch(Z_WMF)\n",
    "        Z_C = torch.squeeze(Z_C, 0).permute([1, 2, 0]).reshape([self.height * self.width, -1])\n",
    "\n",
    "        Z_G = torch.squeeze(Z_WMF, 0).permute([1, 2, 0])\n",
    "        Z_G = self.dropout(Z_G)\n",
    "        Z_G = self.relu(Z_G)\n",
    "        Z_G = torch.reshape(Z_G,[self.height*self.width,Z_G.shape[2]])\n",
    "        # x = self.batch_normalzation1(x)\n",
    "        Z_G = self.HGCN1(Z_G, H)\n",
    "        Z_G = self.dropout(Z_G)\n",
    "        Z_G = self.relu(Z_G)\n",
    "        # x = self.batch_normalzation2(x)\n",
    "        Z_G = self.HGCN2(Z_G, H)\n",
    "\n",
    "        Z_G = self.dropout(Z_G)\n",
    "        Z_G = self.relu(Z_G)\n",
    "        # x = x*(torch.exp(self.lambda1)) + Z_CNN*(1-torch.exp(self.lambda1))\n",
    "        Z = self.lam*Z_G + Z_C*(1-self.lam)\n",
    "        # x = torch.cat((x, Z_CNN), 1)\n",
    "        out = self.Softmax_linear(Z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG = 3 # 1:MUUFL, 2: Houston2013, 3:Trento, 4: Augsburg\n",
    "DataName = \"MUUFL\"\n",
    "max_iters = 10\n",
    "max_epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.00001\n",
    "scales = [30]    \n",
    "k = 1 #如果k=0 使用计算权重\n",
    "pca_n = 15\n",
    "sigma1 = 0.1\n",
    "sigma2 = 0.1\n",
    "if FLAG == 3:\n",
    "    DataName = \"Trento\"\n",
    "    pca_n = 10 #15\n",
    "    lambdas = [0.5] \n",
    "    scales = [100]\n",
    "elif FLAG == 1:\n",
    "    DataName = \"MUUFL\"\n",
    "    pca_n = 10 #30      \n",
    "    lambdas = [0.5]    \n",
    "    scales = [100]  \n",
    "elif FLAG == 2:\n",
    "    DataName = \"Houston2013\"\n",
    "    pca_n = 25   #30\n",
    "    lambdas = [0.5]    \n",
    "    scales = [300]  \n",
    "elif FLAG == 4:\n",
    "    pca_n = 10   \n",
    "    lambdas = [0.5]    \n",
    "    scales = [100] \n",
    "\n",
    "(X_HSI,X_LiDAR, gt, class_num) = get_dataset(FLAG)\n",
    "# no\n",
    "X_HSI = (X_HSI - float(np.min(X_HSI)))\n",
    "X_HSI = X_HSI/(np.max(X_HSI)-np.min(X_HSI))\n",
    "X_LiDAR = (X_LiDAR - float(np.min(X_LiDAR)))\n",
    "X_LiDAR = X_LiDAR/(np.max(X_LiDAR)-np.min(X_LiDAR))\n",
    "X_LiDAR = X_LiDAR[:,:, np.newaxis]\n",
    "#\n",
    "H = obtain_H_from_HSI_with_LiDAR(X_HSI,X_LiDAR,scales)\n",
    "print(H.shape)\n",
    "LiDAR_bands = 1\n",
    "gt_flatten = np.reshape(gt, -1)\n",
    "h,w = X_HSI.shape[0],X_HSI.shape[1]\n",
    "X_HSI = np.reshape(X_HSI,(-1,X_HSI.shape[2]))\n",
    "\n",
    "pca = PCA(n_components=pca_n)  # 指定要保留的主成分数量\n",
    "X_HSI = pca.fit_transform(X_HSI)\n",
    "X_HSI = np.reshape(X_HSI, (h, w, -1))\n",
    "(height, width, bands_hsi) = X_HSI.shape\n",
    "X = np.concatenate([X_HSI,X_LiDAR],axis=2)\n",
    "bands = bands_hsi+1\n",
    "\n",
    "# 将 NumPy 数组转换为 PyTorch 张量\n",
    "X = torch.from_numpy(X).to(device)\n",
    "X = X.type(torch.float32)\n",
    "\n",
    "# 初始化边权矩阵，该参数现有系统为可学习参数，也可设置为固定参数\n",
    "W_e = torch.diag(torch.ones(H.shape[1])).to(device)\n",
    "W_e = W_e.to_sparse_coo()\n",
    "# 将H矩阵转换为边索引的形式\n",
    "idx = np.where(H.T == 1)\n",
    "edge_index = np.stack([idx[1],idx[0]],axis=0)\n",
    "H = torch.tensor(edge_index,dtype=torch.long).to(device)\n",
    "\n",
    "TVT_sets = CTrainValTest_Sets()\n",
    "TVT_sets.get_TrainValTest_Sets(4242,gt,class_num,0.1,0.1,'ratio')    \n",
    "\n",
    "# 使用已存在的训练和测试样本为了保证和其它论文一致\n",
    "if FLAG == 3:\n",
    "    TRLabel = sio.loadmat('data\\Trento\\TrLabel.mat')['TRLabel']\n",
    "    TELabel = sio.loadmat('data\\Trento\\TSLabel.mat')['TSLabel']\n",
    "    \n",
    "elif FLAG == 2:\n",
    "    TRLabel = sio.loadmat('data\\Houston2013\\TrLabel.mat')['Data']\n",
    "    TELabel = sio.loadmat('data\\Houston2013\\TeLabel.mat')['Data']\n",
    "    \n",
    "elif FLAG == 1:\n",
    "    TRLabel = sio.loadmat('data\\MUUFL\\TrLabel.mat')['Data']\n",
    "    TELabel = sio.loadmat('data\\MUUFL\\TeLabel.mat')['Data']\n",
    "    \n",
    "elif FLAG == 4:\n",
    "    TRLabel = sio.loadmat('data\\Augsburg\\TrLabel.mat')['Data']\n",
    "    TELabel = sio.loadmat('data\\Augsburg\\TeLabel.mat')['Data']\n",
    "\n",
    "TELabel_flatten = np.reshape(TELabel,[-1])\n",
    "TRLabel_flatten = np.reshape(TRLabel,[-1])\n",
    "idx = np.where(TRLabel_flatten!=0)\n",
    "TVT_sets.train_data_index = idx[0]\n",
    "TVT_sets.train_gt = torch.tensor(gt_flatten[idx[0]], dtype=torch.long).to(device) - 1\n",
    "\n",
    "TELabel_flatten = np.reshape(TELabel,[-1])\n",
    "idx = np.where(TELabel_flatten!=0)\n",
    "TVT_sets.test_data_index = idx[0]\n",
    "TVT_sets.test_gt = torch.tensor(gt_flatten[idx[0]], dtype=torch.long).to(device) - 1\n",
    "TVT_sets.val_data_index = TVT_sets.test_data_index\n",
    "TVT_sets.val_gt = TVT_sets.test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f498c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OA_ALL = []\n",
    "AA_ALL = []\n",
    "KPP_ALL = []\n",
    "AVG_ALL = []\n",
    "Train_Time_ALL=[]\n",
    "Test_Time_ALL=[]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "W_e = W_e.type(torch.float32)\n",
    "X_torch = X.type(torch.float32)\n",
    "oa_lambda = []\n",
    "oastd_lambda = []\n",
    "print(\"节点数：\",X.shape)\n",
    "print(\"超边数：\",H.shape[1])\n",
    "for lam in lambdas:\n",
    "    \n",
    "    OA_ALL = []\n",
    "    AA_ALL = []\n",
    "    KPP_ALL = []\n",
    "    AVG_ALL = []\n",
    "    Train_Time_ALL=[]\n",
    "    Test_Time_ALL=[]\n",
    "    seed = 1\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    for id in range(max_iters):\n",
    "        # print(\"实验：\",id)\n",
    "        resut_train = CResult()\n",
    "        resut_val = CResult()\n",
    "        resut_test = CResult()\n",
    "        #######################\n",
    "        net = HGCN_Network(height,width,bands,LiDAR_bands,class_num,W_e,lam)\n",
    "        net.to(device)\n",
    "        # loss object\n",
    "        cal_loss = CLoss().to(device)\n",
    "        learning_rate = learning_rate\n",
    "        optimizer = torch.optim.Adam(net.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
    "        net.train()    \n",
    "        \n",
    "        best_loss = 0.5\n",
    "        tic_train = time.time()\n",
    "        for i in range(max_epochs):\n",
    "            # network forward\n",
    "            optimizer.zero_grad()\n",
    "            output= net(X_torch,H)\n",
    "            output_train = output[TVT_sets.train_data_index]\n",
    "            loss = cal_loss(output_train, TVT_sets.train_gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Does the update\n",
    "            if i%10 == 0:\n",
    "                with torch.no_grad():\n",
    "                    net.eval()\n",
    "                    output= net(X_torch,H)\n",
    "                    \n",
    "                    output_train = output[TVT_sets.train_data_index]\n",
    "                    loss_train = cal_loss(output_train, TVT_sets.train_gt)\n",
    "                    resut_train.get_permance(TVT_sets.train_gt, output_train)\n",
    "                    resut_train.lossvalue = loss_train.item()\n",
    "                \n",
    "                    output_val = output[TVT_sets.test_data_index]\n",
    "                    loss_val = cal_loss(output_val, TVT_sets.test_gt)\n",
    "                    resut_val.get_permance(TVT_sets.test_gt, output_val)\n",
    "                    resut_val.lossvalue = loss_val.item()\n",
    "                    if   resut_val.oa > best_loss:\n",
    "                        best_loss = resut_val.oa\n",
    "                        torch.save(net.state_dict(),\"model\\\\\"+str(DataName)+str(id)+\"model.pt\")\n",
    "                torch.cuda.empty_cache()\n",
    "                net.train()\n",
    "                toc1 = time.time()\n",
    "        \n",
    "        toc_train = time.time()\n",
    "        Train_Time_ALL.append(toc_train-tic_train)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            net.load_state_dict(torch.load(\"model\\\\\"+str(DataName)+str(id)+\"model.pt\"))\n",
    "            net.eval()\n",
    "            tic_test = time.time()\n",
    "            output= net(X_torch,H)\n",
    "            toc_test = time.time()\n",
    "            Test_Time_ALL.append(toc_test-tic_test)\n",
    "            \n",
    "            # performance for classification\n",
    "            output_test = output[TVT_sets.test_data_index]\n",
    "            # loss_val = 0 #cal_loss(output_test, TVT_sets.test_gt)\n",
    "            loss_val = cal_loss(output_test, TVT_sets.test_gt)\n",
    "            # testgtnumpy = TVT_sets.test_gt.detach().cpu().numpy()\n",
    "            resut_test.get_permance(TVT_sets.test_gt, output_test)\n",
    "            print(\"test_loss:{} \\toa_test:{}\".format(loss_val, resut_test.oa))\n",
    "            \n",
    "            # testOA, testAA, testkappa\n",
    "            OA_ALL.append(resut_test.oa)\n",
    "            AA_ALL.append(resut_test.aa)\n",
    "            KPP_ALL.append(resut_test.kappa)\n",
    "            AVG_ALL.append(resut_test.acc_perclass)\n",
    "            # print(net.HGCN1.W_e)            \n",
    "        torch.cuda.empty_cache()\n",
    "        # del net\n",
    "    OA_ALL = np.array(OA_ALL)\n",
    "    AA_ALL = np.array(AA_ALL)\n",
    "    KPP_ALL = np.array(KPP_ALL)\n",
    "    AVG_ALL = np.array(AVG_ALL)\n",
    "    Train_Time_ALL=np.array(Train_Time_ALL)\n",
    "    Test_Time_ALL=np.array(Test_Time_ALL)\n",
    "\n",
    "    print(\"\\ntrain_ratio={}\".format(1),\n",
    "            \"\\n==============================================\")\n",
    "    print('OA=', np.mean(OA_ALL), '+-', np.std(OA_ALL))\n",
    "    print('AA=', np.mean(AA_ALL), '+-', np.std(AA_ALL))\n",
    "    print('Kpp=', np.mean(KPP_ALL), '+-', np.std(KPP_ALL))\n",
    "    print('AVG=', np.mean(AVG_ALL, 0), '+-', np.std(AVG_ALL, 0))\n",
    "    print(\"Average training time:{}\".format(np.mean(Train_Time_ALL)))\n",
    "    print(\"Average testing time:{}\".format(np.mean(Test_Time_ALL)))\n",
    "    oa_lambda.append(np.mean(OA_ALL))\n",
    "    oastd_lambda.append(np.std(OA_ALL))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
